{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IW0PQ1J7NN3c"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "# Create a label map\n",
    "label_list = ['supp_name', 'supp_vat', 'date', 'amount']\n",
    "label_list_B = [f'B-{label}' for label in label_list]\n",
    "label_list_I = [f'I-{label}' for label in label_list]\n",
    "label_list_BIO = ['O'] + [elem for pair in zip(label_list_B, label_list_I) for elem in pair]\n",
    "\n",
    "label2id = {label: i for i, label in enumerate(label_list_BIO)}\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "num_labels = len(label2id)\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "86vgz5JFNN3e"
   },
   "outputs": [],
   "source": [
    "# Align the labels\n",
    "def align_labels_with_tokens(labels, word_ids):\n",
    "    new_labels = []\n",
    "    current_word = None\n",
    "    for word_id in word_ids:\n",
    "        if word_id != current_word:\n",
    "            # Start of a new word!\n",
    "            current_word = word_id\n",
    "            label = -100 if word_id is None else labels[word_id]\n",
    "            new_labels.append(label)\n",
    "        elif word_id is None:\n",
    "            # Special token\n",
    "            new_labels.append(-100)\n",
    "        else:\n",
    "            # Same word as previous token\n",
    "            label = labels[word_id]\n",
    "            # If the label is B-XXX we change it to I-XXX\n",
    "            if label % 2 == 1:\n",
    "                label += 1\n",
    "            new_labels.append(label)\n",
    "\n",
    "    return new_labels\n",
    "\n",
    "\n",
    "def prepare_data(data):\n",
    "    # Fix the format of the 'amount' column to always have two decimal places\n",
    "    data['amount'] = data['amount'].astype(float).apply(lambda x: f\"{x:.2f}\")\n",
    "    # Convert all columns to string to handle numeric values\n",
    "    data = data.astype(str)\n",
    "\n",
    "    # Prepare the data\n",
    "    sentences = data['text_extract'].tolist()\n",
    "    supp_names = data['supp_name'].tolist()\n",
    "    supp_vats = data['supp_vat'].tolist()\n",
    "    dates = data['date'].tolist()\n",
    "    amounts = data['amount'].tolist()\n",
    "\n",
    "    tokenized_data = []\n",
    "    labels_full = []\n",
    "\n",
    "    def find_entity_indices(text, entity, entity_type):\n",
    "        patterns = [re.escape(entity)]\n",
    "        if entity_type == 'date':\n",
    "            if len(entity) == 10:  # DD.MM.YYYY\n",
    "                short_year = entity[-2:]\n",
    "                short_entity = entity[:-4] + short_year\n",
    "                patterns.append(re.escape(short_entity))\n",
    "            elif len(entity) == 8:  # DD.MM.YY\n",
    "                long_entity = entity[:-2] + '20' + entity[-2:]\n",
    "                patterns.append(re.escape(long_entity))\n",
    "        elif entity_type == 'amount':\n",
    "            if ',' in entity:\n",
    "                patterns.append(re.escape(entity.replace(',', '.')))\n",
    "            elif '.' in entity:\n",
    "                patterns.append(re.escape(entity.replace('.', ',')))\n",
    "\n",
    "        for pattern in patterns:\n",
    "            match = re.search(pattern, text)\n",
    "            if match:\n",
    "                return match.start(), match.end()\n",
    "        return None, None\n",
    "\n",
    "    for sentence, supp_name, supp_vat, date, amount in zip(sentences, supp_names, supp_vats, dates, amounts):\n",
    "        words = sentence.split()\n",
    "        labels = ['O'] * len(words)\n",
    "\n",
    "        # Assign labels to each word in the sentence\n",
    "        for entity, label_prefix in zip([supp_name, supp_vat, date, amount],\n",
    "                                        ['supp_name', 'supp_vat', 'date', 'amount']):\n",
    "            if entity:\n",
    "                start, end = find_entity_indices(sentence, entity, label_prefix)\n",
    "                if start is not None:\n",
    "                    start_idx = len(sentence[:start].split())\n",
    "                    end_idx = len(sentence[:end].split())\n",
    "                    labels[start_idx] = f'B-{label_prefix}'\n",
    "                    for i in range(start_idx + 1, end_idx):\n",
    "                        labels[i] = f'I-{label_prefix}'\n",
    "        # print(sentence, '\\n', labels)\n",
    "        # Tokenize the sentence and align labels with subwords\n",
    "        labels_ids = [label2id[label] for label in labels]\n",
    "        # tokenized_inputs = tokenizer(words, truncation=True, is_split_into_words=True)\n",
    "        tokenized_inputs = tokenizer(words, padding='max_length', truncation=True, is_split_into_words=True)\n",
    "        aligned_labels = align_labels_with_tokens(labels_ids, tokenized_inputs.word_ids())\n",
    "        tokenized_inputs[\"labels\"] = aligned_labels\n",
    "        tokenized_data.append(tokenized_inputs)\n",
    "        labels_full.append(labels)\n",
    "\n",
    "    return tokenized_data, labels_full\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "4ILPPBG-NN3f"
   },
   "outputs": [],
   "source": [
    "# Prepare the data\n",
    "df_train = pd.read_csv('data_train.csv')\n",
    "df_test1 = pd.read_csv('data_test1.csv')\n",
    "df_test2 = pd.read_csv('data_test2.csv')\n",
    "\n",
    "train_data, labels_full = prepare_data(df_train)\n",
    "# df_train['labels_full'] = labels_full\n",
    "test1_data, labels_full = prepare_data(df_test1)\n",
    "# df_test1['labels_full'] = labels_full\n",
    "test2_data, labels_full = prepare_data(df_test2)\n",
    "# df_test2['labels_full'] = labels_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f728AIJJNN3f",
    "outputId": "8080bd11-9603-4fda-dece-612975989c79"
   },
   "outputs": [],
   "source": [
    "print(type(train_data))\n",
    "print(train_data[0])\n",
    "print(train_data[0]['input_ids'])\n",
    "print(train_data[0]['labels'])\n",
    "print(train_data[0]['attention_mask'])\n",
    "# print(df_train['labels_full'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "U4AiT2ubNN3g"
   },
   "outputs": [],
   "source": [
    "# !pip install evaluate\n",
    "# !pip install seqeval\n",
    "\n",
    "import numpy as np\n",
    "import evaluate\n",
    "from transformers import AutoModelForTokenClassification, TrainingArguments, DataCollatorForTokenClassification, Trainer\n",
    "\n",
    "# Load the seqeval metric\n",
    "metric = evaluate.load(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "L_09A1iRNN3g"
   },
   "outputs": [],
   "source": [
    "# Define a compute_metrics function using seqeval\n",
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    # Remove ignored index (special tokens) and convert to labels\n",
    "    true_labels = [[id2label[l] for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [\n",
    "        [id2label[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    all_metrics = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"accuracy\": all_metrics[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "g6u2RssiNN3g"
   },
   "outputs": [],
   "source": [
    "# Function to train and evaluate model\n",
    "def train_and_eval(train_data, eval_data, label, num_labels):\n",
    "    model = AutoModelForTokenClassification.from_pretrained(\n",
    "        \"bert-base-cased\",\n",
    "        num_labels=num_labels\n",
    "    )\n",
    "\n",
    "    args = TrainingArguments(\n",
    "        f\"bert-finetuned-ner-{label}\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        learning_rate=2e-5,\n",
    "        num_train_epochs=10,  # Adjust as needed\n",
    "        weight_decay=0.01,\n",
    "    )\n",
    "\n",
    "    data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=train_data,\n",
    "        eval_dataset=eval_data,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    eval_results = trainer.evaluate()\n",
    "    print(f\"Evaluation results for {label}: {eval_results}\")\n",
    "\n",
    "    return trainer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JsRK-dD-NN3h"
   },
   "source": [
    "## Multiclass classification - all entity prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "U71p_jZaNN3i"
   },
   "outputs": [],
   "source": [
    "def true_and_predicted_words_for_all_labels(predictions, label_ids, test_data):\n",
    "    predictions = np.argmax(predictions, axis=-1)\n",
    "\n",
    "    # Remove ignored index (special tokens) and convert to labels\n",
    "    true_labels = [[l for l in label if l != -100] for label in label_ids]\n",
    "    predicted_labels = [\n",
    "        [p for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, label_ids)\n",
    "    ]\n",
    "\n",
    "    true_words_dict = {label: [] for label in label_list}\n",
    "    pred_words_dict = {label: [] for label in label_list}\n",
    "\n",
    "    for true_labels_row, pred_labels_row, token_ids in zip(true_labels, predicted_labels, [entry['input_ids'] for entry in test_data]):\n",
    "        tokens = tokenizer.convert_ids_to_tokens(token_ids, skip_special_tokens=True)\n",
    "\n",
    "        for label in label_list:\n",
    "            B_label_id = label2id[f'B-{label}']\n",
    "            I_label_id = label2id[f'I-{label}']\n",
    "\n",
    "            true_entity = None\n",
    "            pred_entity = None\n",
    "            current_true_entity = []\n",
    "            current_pred_entity = []\n",
    "\n",
    "            for token, true_label, pred_label in zip(tokens, true_labels_row, pred_labels_row):\n",
    "                if true_label == B_label_id or true_label == I_label_id:\n",
    "                    current_true_entity.append(token)\n",
    "                elif current_true_entity:\n",
    "                    if not true_entity:\n",
    "                        true_entity = tokenizer.convert_tokens_to_string(current_true_entity)\n",
    "                    current_true_entity = []\n",
    "\n",
    "                if pred_label == B_label_id or pred_label == I_label_id:\n",
    "                    current_pred_entity.append(token)\n",
    "                elif current_pred_entity:\n",
    "                    if not pred_entity:\n",
    "                        pred_entity = tokenizer.convert_tokens_to_string(current_pred_entity)\n",
    "                    current_pred_entity = []\n",
    "\n",
    "                if true_entity and pred_entity:\n",
    "                    break  # Break the loop once both entities are found\n",
    "\n",
    "            if current_true_entity and not true_entity:\n",
    "                true_entity = tokenizer.convert_tokens_to_string(current_true_entity)\n",
    "            if current_pred_entity and not pred_entity:\n",
    "                pred_entity = tokenizer.convert_tokens_to_string(current_pred_entity)\n",
    "\n",
    "            true_words_dict[label].append(true_entity if true_entity else \"\")\n",
    "            pred_words_dict[label].append(pred_entity if pred_entity else \"\")\n",
    "\n",
    "    return true_words_dict, pred_words_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "_77-doURNN3j",
    "outputId": "4321c0dd-8ddf-4197-dd24-4f6bce097ac6"
   },
   "outputs": [],
   "source": [
    "# Train model for all labels together (multiclass classification)\n",
    "\n",
    "# !pip install transformers[torch]\n",
    "trainer_all_labels = train_and_eval(train_data, test1_data, 'all_labels', num_labels=len(label_list_BIO))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "c69f54d027034819ab39d7c32dfcb9ac",
      "207b00e1f49f4f4ba8cda33abff6a1da"
     ]
    },
    "id": "oLkwGNeyNN3j",
    "outputId": "4a85b262-aeae-4d8c-d312-28599f9da473"
   },
   "outputs": [],
   "source": [
    "print(\"Predicting for test1_data with all labels...\")\n",
    "predictions_all_labels, labels_all_labels, metrics_all_labels = trainer_all_labels.predict(test1_data)\n",
    "# reassemble the predicted words\n",
    "true_words_dict, pred_words_dict = true_and_predicted_words_for_all_labels(predictions_all_labels, labels_all_labels, test1_data)\n",
    "# Add the predicted words for each label as new columns\n",
    "for label in label_list:\n",
    "    df_test1[f'{label}_pred_multiclass'] = pred_words_dict[label]\n",
    "\n",
    "# Remove all spaces from the entries of each prediction column\n",
    "pattern = r\"\\s*'\\s*|\\s*-\\s*\"\n",
    "for label in ['supp_vat', 'date', 'amount']:\n",
    "    df_test1[f'{label}_pred_multiclass'] = df_test1[f'{label}_pred_multiclass'].str.replace(' ', '')\n",
    "\n",
    "# Function to remove spaces around ', - (McDonald's e.g.)\n",
    "def remove_spaces_around_characters(text):\n",
    "    pattern = r\"\\s*'\\s*|\\s*-\\s*\"\n",
    "    return re.sub(pattern, lambda x: x.group(0).replace(' ', ''), text)\n",
    "\n",
    "df_test1['supp_name_pred_multiclass'] = df_test1['supp_name_pred_multiclass'].apply(remove_spaces_around_characters)\n",
    "print(df_test1.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UlyeTWE2NN3k",
    "outputId": "c3e828b7-249d-4b48-a4a6-17131f0d9d0a"
   },
   "outputs": [],
   "source": [
    "information_extractions = []\n",
    "for i in range(df_test1.shape[0]):\n",
    "  extraction_result = {\n",
    "            'Supplier company name': [],\n",
    "            'Supplier VAT number': [],\n",
    "            'Date': [],\n",
    "            'Amount': []\n",
    "        }\n",
    "  extraction_result['Supplier company name'] = df_test1['supp_name_pred_multiclass'].values[i]\n",
    "  extraction_result['Supplier VAT number'] = df_test1['supp_vat_pred_multiclass'].values[i]\n",
    "  extraction_result['Date'] = df_test1['date_pred_multiclass'].values[i]\n",
    "  extraction_result['Amount'] = df_test1['amount_pred_multiclass'].values[i]\n",
    "  information_extractions.append(extraction_result)\n",
    "\n",
    "# Save extraction results\n",
    "result_file_name = \"test1_bert.pickle\"\n",
    "with open(result_file_name, \"wb\") as file:\n",
    "    pickle.dump(information_extractions, file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OK8IYlvsR5q1"
   },
   "outputs": [],
   "source": [
    "print(\"Predicting for test2_data with all labels...\")\n",
    "predictions_all_labels, labels_all_labels, metrics_all_labels = trainer_all_labels.predict(test2_data)\n",
    "# reassemble the predicted words\n",
    "true_words_dict, pred_words_dict = true_and_predicted_words_for_all_labels(predictions_all_labels, labels_all_labels, test2_data)\n",
    "# Add the predicted words for each label as new columns\n",
    "for label in label_list:\n",
    "    df_test2[f'{label}_pred_multiclass'] = pred_words_dict[label]\n",
    "\n",
    "# Remove all spaces from the entries of each prediction column\n",
    "for label in ['supp_vat', 'date', 'amount']:\n",
    "    df_test2[f'{label}_pred_multiclass'] = df_test2[f'{label}_pred_multiclass'].str.replace(' ', '')\n",
    "\n",
    "# Function to remove spaces around ', - (McDonald's e.g.)\n",
    "def remove_spaces_around_characters(text):\n",
    "    pattern = r\"\\s*'\\s*|\\s*-\\s*\"\n",
    "    return re.sub(pattern, lambda x: x.group(0).replace(' ', ''), text)\n",
    "\n",
    "df_test2['supp_name_pred_multiclass'] = df_test2['supp_name_pred_multiclass'].apply(remove_spaces_around_characters)\n",
    "print(df_test2.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FZ4WG52AR54R"
   },
   "outputs": [],
   "source": [
    "information_extractions = []\n",
    "for i in range(df_test2.shape[0]):\n",
    "  extraction_result = {\n",
    "            'Supplier company name': [],\n",
    "            'Supplier VAT number': [],\n",
    "            'Date': [],\n",
    "            'Amount': []\n",
    "        }\n",
    "  extraction_result['Supplier company name'] = df_test2['supp_name_pred_multiclass'].values[i]\n",
    "  extraction_result['Supplier VAT number'] = df_test2['supp_vat_pred_multiclass'].values[i]\n",
    "  extraction_result['Date'] = df_test2['date_pred_multiclass'].values[i]\n",
    "  extraction_result['Amount'] = df_test2['amount_pred_multiclass'].values[i]\n",
    "  information_extractions.append(extraction_result)\n",
    "\n",
    "# Save extraction results\n",
    "result_file_name = \"test2_bert.pickle\"\n",
    "with open(result_file_name, \"wb\") as file:\n",
    "    pickle.dump(information_extractions, file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
